{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from scipy import spatial\n",
    "from pmaw import PushshiftAPI\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word = str\n",
    "Sentence = List[Word]\n",
    "\n",
    "reddit_data_path = Path(\"data/reddit_data.txt\")\n",
    "essais_politiques_path = Path(\"data/quinze_essais_politiques.txt\")\n",
    "model_path = \"models/doc2vec_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists\n"
     ]
    }
   ],
   "source": [
    "#scrapping reddit data and store it in a txt file\n",
    "\n",
    "def get_reddit_data(nbPost: int) -> None:\n",
    "    api = PushshiftAPI()\n",
    "    comments = api.search_submissions(subreddit=\"Politiquefrancaise\", limit=nbPost, TimeoutError=1000)\n",
    "    \n",
    "    comment_list = [comment['selftext'] for comment in comments]\n",
    "    with open(reddit_data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for comment in comment_list:\n",
    "            #remove urls\n",
    "            comment = re.sub(r'http\\S+', '', comment)\n",
    "            f.write(comment)\n",
    "    return\n",
    "\n",
    "if not reddit_data_path.exists():\n",
    "    print(\"Creating reddit data file\")\n",
    "    get_reddit_data(10000)\n",
    "else:\n",
    "    print(\"Data already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus 1 has 4723 sentences\n",
      "['avertissement', 'du', 'traducteur', 'retour', 'à', 'la', 'table', 'des', 'matières', 'ce', 'fichier']\n",
      "['malgré', 'son', 'titre']\n",
      "['ne', 'comporte', 'pas', 'tous', 'les', 'essais', 'des', 'political', 'discourses', '1', 'de', '1752', 'edinburgh']\n",
      "['kincaid', 'et', 'donaldson', 'dont', 'la', 'plupart', 'ont', 'déjà', 'été', 'traduits', 'par', 'nos', 'soins', 'et', 'intégrés', 'au', 'fichier', 'essais', 'économiques', 'de', 'hume', 'paru', 'en', 'août', '2007', 'aux', 'classiques', 'des', 'sciences', 'sociales']\n",
      "['ce', 'présent', 'fichier', 'contient', 'un', 'choix', 'dessais', 'qui', 'traitent', 'plus', 'directement', 'de', 'questions', 'politiques']\n",
      "['david', 'hume', 'essai', 'sur', 'la', 'liberté', 'de', 'la', 'presse']\n",
      "['of', 'the', 'liberty', 'of', 'the', 'press', 'in', 'essays']\n",
      "['moral', 'and', 'political', '1', 'volume', 'edinburgh']\n",
      "['a']\n",
      "['kincaid', '1741', 'traduction', 'de', 'philippe', 'folliot']\n",
      "The corpus 2 has 4085 sentences\n",
      "['le', 'président', 'emmanuel', 'macron', 'a', 'appelé', 'dans', 'un', 'discours', 'devant', 'le', 'parlement', 'européen', 'à', 'proposer', 'une', 'nouvelle', 'alliance', 'avec', 'les', 'pays', 'africains', 'en', 'matière', 'dinvestissements']\n",
      "['de', 'santé', 'et', 'de', 'sécurité']\n",
      "['macron', 'a', 'annoncé', 'que', 'la', 'france', 'organisera', 'un', 'sommet', 'europeafrique', 'en', 'février']\n",
      "['dans', 'le', 'but', 'de', 'reconstruire', 'le', 'partenariat', 'entre', 'les', 'deux', 'continents']\n",
      "['cet', 'appel', 'est', 'venu', 'dans', 'un', 'discours', 'dans', 'lequel', 'il', 'a', 'passé', 'en', 'revue', 'les', 'priorités', 'de', 'la', 'france', 'à', 'strasbourg', 'à', 'loccasion', 'de', 'sa', 'présidence', 'de', 'lunion', 'européenne', 'au', 'cours', 'du', 'premier', 'semestre', '2022']\n",
      "['dans', 'lequel', 'il', 'a', 'évoqué', 'la', 'démocratie']\n",
      "['lenvironnement', 'et', 'la', 'paix', 'dans', 'le', 'vieux', 'continent']\n",
      "['macron', 'a', 'souligné', 'que', 'les', 'européens', 'devaient', 'établir', 'un', 'nouvel', 'ordre', 'de', 'sécurité', 'et', 'de', 'stabilité', 'qui', 'appelle', 'à', 'un', 'réarmement', 'stratégique', 'et', 'à', 'des', 'pourparlers', 'francs', 'avec', 'la', 'russie']\n",
      "['le', 'premier', 'ministre', 'jean', 'castex', 'a', 'annoncé', 'il', 'y', 'a', 'quelques', 'jours']\n",
      "['lors', 'dune', 'conférence', 'de', 'presse', 'conjointe', 'avec', 'le', 'ministre', 'de', 'la', 'santé', 'olivier', 'veran']\n"
     ]
    }
   ],
   "source": [
    "#télcharge \"quinze essais politiques\" dans data et en fait une liste\n",
    "# opening the file in read mode\n",
    "\n",
    "def read_text_data(file_path: Path, encoding=\"\") -> List[Sentence]:\n",
    "    if encoding != \"\":\n",
    "        with open(file_path, \"r\", encoding=encoding) as f:\n",
    "            corpus = f.read()\n",
    "    else:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "\n",
    "    corpus = corpus.replace('\\n', ' ') # put text on one line\n",
    "    corpus = corpus.replace('.', ',') # text separated by commas will be considered as different sentences\n",
    "\n",
    "    sentences_list = corpus.split(\",\") # split the text into a list of sentences\n",
    "    \n",
    "    for i in range(len(sentences_list)):\n",
    "        sentences_list[i] = sentences_list[i].lower() # change all characters to lower\n",
    "        sentences_list[i] = re.sub(r'[^\\w\\s]','',sentences_list[i]) # remove punctuation\n",
    "        sentences_list[i] = sentences_list[i].split() # split every sentence into a list of words\n",
    "        \n",
    "    return sentences_list\n",
    "\n",
    "corpus1 = read_text_data(essais_politiques_path)\n",
    "corpus2 = read_text_data(reddit_data_path, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"The corpus 1 has {len(corpus1)} sentences\")\n",
    "for sentence in corpus1[:10]:\n",
    "    print(sentence)\n",
    "    \n",
    "print(f\"The corpus 2 has {len(corpus2)} sentences\")\n",
    "for sentence in corpus2[:10]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the doc2vec model, input data should be in format of iterable TaggedDocuments\"\n",
    "\n",
    "Each TaggedDocument instance comprises words and tags\n",
    "\n",
    "Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is identifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument<['le', 'président', 'emmanuel', 'macron', 'a', 'appelé', 'dans', 'un', 'discours', 'devant', 'le', 'parlement', 'européen', 'à', 'proposer', 'une', 'nouvelle', 'alliance', 'avec', 'les', 'pays', 'africains', 'en', 'matière', 'dinvestissements'], ['sent0']>\n"
     ]
    }
   ],
   "source": [
    "def tag_text(sentences: List[Sentence]) -> List[TaggedDocument]:\n",
    "    \"\"\"converting each sentence into a TaggedDocument\"\"\"\n",
    "    tagged_docs = []\n",
    "    for i in range(len(sentences)):\n",
    "        tagged_docs.append(TaggedDocument(words = sentences[i], tags = ['sent{}'.format(i)]))\n",
    "    return tagged_docs\n",
    "\n",
    "tagged_c1 = tag_text(corpus2)\n",
    "print(tagged_c1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\antoi\\AppData\\Local\\Temp/ipykernel_22320/2127870977.py:4: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace = True)\n"
     ]
    }
   ],
   "source": [
    "corpus = tagged_c1\n",
    "\n",
    "model = Doc2Vec(documents = corpus, vector_size = 10, min_count = 1)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model.save(model_path)\n",
    "model = Doc2Vec.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18243521451950073"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.infer_vector(['front national'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector(['insoumis'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "cosine_similarity(v1, v2)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
