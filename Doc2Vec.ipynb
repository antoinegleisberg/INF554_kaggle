{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from scipy import spatial\n",
    "from pmaw import PushshiftAPI\n",
    "from pathlib import Path\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word = str\n",
    "Sentence = List[Word]\n",
    "\n",
    "reddit_data_path = Path(\"data/reddit_data.txt\")\n",
    "model_path = \"doc2vec_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists\n"
     ]
    }
   ],
   "source": [
    "#scrapping reddit data and store it in a txt file\n",
    "\n",
    "def get_reddit_data(nbPost: int) -> None:\n",
    "    api = PushshiftAPI()\n",
    "    comments = api.search_submissions(subreddit=\"Politiquefrancaise\", limit=nbPost, TimeoutError=1000)\n",
    "    \n",
    "    comment_list = [comment['selftext'] for comment in comments]\n",
    "    with open(reddit_data_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for comment in comment_list:\n",
    "            #remove urls\n",
    "            comment = re.sub(r'http\\S+', '', comment)\n",
    "            f.write(comment)\n",
    "    return\n",
    "\n",
    "if not reddit_data_path.exists():\n",
    "    print(\"Creating reddit data file\")\n",
    "    get_reddit_data(10000)\n",
    "else:\n",
    "    print(\"Data already exists\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the doc2vec model, input data should be in format of iterable TaggedDocuments\"\n",
    "\n",
    "Each TaggedDocument instance comprises words and tags\n",
    "\n",
    "Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is identifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#télcharge \"quinze essais politiques\" dans data et en fait une liste\n",
    "# opening the file in read mode\n",
    "\n",
    "def read_text_data(file_path: Path, encoding=\"\") -> List[Sentence]:\n",
    "    if encoding != \"\":\n",
    "        with open(file_path, \"r\", encoding=encoding) as f:\n",
    "            corpus = f.read()\n",
    "    else:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            corpus = f.read()\n",
    "\n",
    "    corpus = corpus.replace('\\n', ' ') # put text on one line\n",
    "    corpus = corpus.replace('.', ',') # text separated by commas will be considered as different sentences\n",
    "\n",
    "    sentences_list = corpus.split(\",\") # split the text into a list of sentences\n",
    "    \n",
    "    for i in range(len(sentences_list)):\n",
    "        sentences_list[i] = sentences_list[i].lower() # change all characters to lower\n",
    "        sentences_list[i] = re.sub(r'[^\\w\\s]','',sentences_list[i]) # remove punctuation\n",
    "        sentences_list[i] = sentences_list[i].split() # split every sentence into a list of words\n",
    "        \n",
    "    return sentences_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The corpus 2 has 4098 sentences\n",
      "['la', 'france', 'connaîtra', 'une', 'augmentation', 'rapide', 'des', 'cas', 'de', 'contamination', 'a', 'la', 'covid19', 'dans', 'les', 'prochains', 'jours']\n",
      "['a', 'déclaré', 'hier', 'gabriel', 'attal']\n",
      "['porteparole', 'du', 'gouvernement', 'français']\n",
      "['ajoutant', 'que', 'rien', 'nindique', 'que', 'le', 'nombre', 'de', 'cas', 'vas', 'diminuer']\n",
      "['gabriel', 'attal', 'a', 'déclaré', 'que', 'les', 'nouveaux', 'cas', 'avaient', 'atteint', 'des', 'niveaux', 'extrêmement', 'élevés', 'dans', 'la', 'région', 'iledefrance', 'entourant', 'paris', 'et', 'certaines', 'autres', 'régions', 'du', 'pays']\n",
      "['tout', 'en', 'notant', 'que', 'la', 'situation', 'dans', 'les', 'hôpitaux', 'pourrait', 'se', 'détériorer', 'dans', 'les', 'semaines', 'à', 'venir']\n",
      "['les', 'réactions', 'se', 'poursuivent', 'toujours', 'en', 'france', 'suite', 'aux', 'déclarations', 'faites', 'mardi', 'par', 'le', 'président', 'emmanuel', 'macron', 'au', 'journal', 'le', 'parisien']\n",
      "['dans', 'lesquelles', 'il', 'a', 'déclaré', 'jai', 'vraiment', 'envie', 'demmerder', 'les', 'non', 'vaccinées']\n",
      "['par', 'conséquent']\n",
      "['nous', 'continuerons', 'à', 'le', 'faire', 'jusquà', 'la', 'fin']\n",
      "TaggedDocument(['la', 'france', 'connaîtra', 'une', 'augmentation', 'rapide', 'des', 'cas', 'de', 'contamination', 'a', 'la', 'covid19', 'dans', 'les', 'prochains', 'jours'], ['sent0'])\n"
     ]
    }
   ],
   "source": [
    "def tag_text(sentences: List[Sentence]) -> List[TaggedDocument]:\n",
    "    \"\"\"converting each sentence into a TaggedDocument\"\"\"\n",
    "    tagged_docs = []\n",
    "    for i in range(len(sentences)):\n",
    "        tagged_docs.append(TaggedDocument(words = sentences[i], tags = ['sent{}'.format(i)]))\n",
    "    return tagged_docs\n",
    "corpus = read_text_data(reddit_data_path, encoding=\"utf-8\")\n",
    "print(f\"The corpus 2 has {len(corpus)} sentences\")\n",
    "for sentence in corpus[:10]:\n",
    "    print(sentence)\n",
    "tagged_c1 = tag_text(corpus)\n",
    "print(tagged_c1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maria\\AppData\\Local\\Temp\\ipykernel_9772\\2773423615.py:4: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  model.init_sims(replace = True)\n"
     ]
    }
   ],
   "source": [
    "corpus = tagged_c1\n",
    "\n",
    "model = Doc2Vec(documents = corpus, vector_size = 100, min_count = 1)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model.save(model_path)\n",
    "model = Doc2Vec.load(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.14759613573551178\n",
      "-0.004457784816622734\n",
      "-0.0378972664475441\n"
     ]
    }
   ],
   "source": [
    "v1 = model.infer_vector(['votez macron'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector(['macron quitte gouvernement'])\n",
    "v3= model.infer_vector(['zemmour président'])\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "print(cosine_similarity(v1, v2))\n",
    "print(cosine_similarity(v1, v3))\n",
    "print(cosine_similarity(v2, v3))\n",
    "\n",
    "#V2 and v3 should be the most similar vectors but it is not the case\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2963140308856964\n",
      "0.3943708837032318\n",
      "0.5074030756950378\n",
      "96\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('fr_core_news_sm')# python -m spacy download fr_core_news_sm\n",
    "# process a sentence using the model\n",
    "doc1 = nlp(\"votez macron\")\n",
    "doc2 = nlp(\"Macron quitte gouvernement\")\n",
    "doc3 = nlp(\"zemmour président\")\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "# Get the vector for 'text':\n",
    "vs1=doc1.vector\n",
    "vs2=doc2.vector\n",
    "vs3=doc3.vector\n",
    "print(cosine_similarity(vs1, vs2))\n",
    "print(cosine_similarity(vs1, vs3))\n",
    "print(cosine_similarity(vs2, vs3))\n",
    "\n",
    "#Spacy is able to see similarity between v2 and V3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
