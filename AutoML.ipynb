{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook magique with AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\maria\\anaconda3\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sns as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.1, test_size=0.9)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mentions is always empty -> DROP Mention\n",
    "\n",
    "X_train = X_train.drop(['mentions'], axis=1)\n",
    "X_test = X_test.drop(['mentions'], axis=1)\n",
    "\n",
    "#Tweet id is not relevant -> DROP Tweet id\n",
    "X_train = X_train.drop(['TweetID'], axis=1)\n",
    "X_test = X_test.drop(['TweetID'], axis=1)\n",
    "\n",
    "\n",
    "#add a column to data which counts url\n",
    "X_train['url_count'] = X_train['urls'].str.count('http')\n",
    "X_test['url_count'] = X_test['urls'].str.count('http')\n",
    "X_train = X_train.drop(['urls'], axis=1)\n",
    "X_test = X_test.drop(['urls'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#drop the text column\n",
    "X_train = X_train.drop(['text'], axis=1)\n",
    "X_test = X_test.drop(['text'], axis=1)\n",
    "\n",
    "#drop the hashtags column\n",
    "X_train = X_train.drop(['hashtags'], axis=1)\n",
    "X_test = X_test.drop(['hashtags'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3eda37e9771848be91ea5787cd42c588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/70 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: -7.392794256528634\n",
      "\n",
      "Generation 2 - Current best internal CV score: -7.392794256528634\n",
      "\n",
      "Generation 3 - Current best internal CV score: -6.982254332261036\n",
      "\n",
      "Generation 4 - Current best internal CV score: -6.954074750846345\n",
      "\n",
      "Generation 5 - Current best internal CV score: -6.913547259316941\n",
      "\n",
      "Generation 6 - Current best internal CV score: -6.913547259316941\n",
      "\n",
      "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.7500000000000001, min_samples_leaf=4, min_samples_split=7, n_estimators=100)\n",
      "-6.837551911348925\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor #conda install -c conda-forge tpot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "pipeline_optimizer = TPOTRegressor(generations=6, population_size=10, cv=5,\n",
    "                                    random_state=42, verbosity=2, scoring='neg_mean_absolute_error')\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_test, y_test))\n",
    "pipeline_optimizer.export('tpot_exported_pipeline.py')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generations=3, population_size=6, 90min\n",
    "\n",
    "Best pipeline: RandomForestRegressor(MinMaxScaler(input_matrix), bootstrap=True, max_features=0.4, min_samples_leaf=6, min_samples_split=14, n_estimators=100)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "# NOTE: Make sure that the outcome column is labeled 'target' in the data file\n",
    "tpot_data = pd.read_csv('PATH/TO/DATA/FILE', sep='COLUMN_SEPARATOR', dtype=np.float64)\n",
    "features = tpot_data.drop('target', axis=1)\n",
    "training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=42)\n",
    "\n",
    "# Average CV score on the training set was: -6.869999442017312\n",
    "exported_pipeline = make_pipeline(\n",
    "    MinMaxScaler(),\n",
    "    RandomForestRegressor(bootstrap=True, max_features=0.4, min_samples_leaf=6, min_samples_split=14, n_estimators=100)\n",
    ")\n",
    "# Fix random state for all the steps in exported pipeline\n",
    "set_param_recursive(exported_pipeline.steps, 'random_state', 42)\n",
    "\n",
    "exported_pipeline.fit(training_features, training_target)\n",
    "results = exported_pipeline.predict(testing_features)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Best pipeline: RandomForestRegressor(input_matrix, bootstrap=True, max_features=0.7500000000000001, min_samples_leaf=4, min_samples_split=7, n_estimators=100)\n",
    "-6.837551911348925\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
