{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook magique with AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_data.drop(['retweets_count'], axis=1), train_data['retweets_count'], train_size=0.7, test_size=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mentions is always empty -> DROP Mention\n",
    "\n",
    "X_train = X_train.drop(['mentions'], axis=1)\n",
    "X_test = X_test.drop(['mentions'], axis=1)\n",
    "\n",
    "#Tweet id is not relevant -> DROP Tweet id\n",
    "X_train = X_train.drop(['TweetID'], axis=1)\n",
    "X_test = X_test.drop(['TweetID'], axis=1)\n",
    "\n",
    "\n",
    "#add a column to data which counts url\n",
    "X_train['url_count'] = X_train['urls'].str.count('http')\n",
    "X_test['url_count'] = X_test['urls'].str.count('http')\n",
    "X_train = X_train.drop(['urls'], axis=1)\n",
    "X_test = X_test.drop(['urls'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#drop the text column\n",
    "X_train = X_train.drop(['text'], axis=1)\n",
    "X_test = X_test.drop(['text'], axis=1)\n",
    "\n",
    "#drop the hashtags column\n",
    "X_train = X_train.drop(['hashtags'], axis=1)\n",
    "X_test = X_test.drop(['hashtags'], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d3a5adf3314e3fa65d1c8e9c1e524e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/60 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "TPOT closed during evaluation in one generation.\n",
      "WARNING: TPOT may not provide a good pipeline if TPOT is stopped/interrupted in a early generation.\n",
      "\n",
      "\n",
      "TPOT closed prematurely. Will use the current best pipeline.\n"
     ]
    }
   ],
   "source": [
    "from tpot import TPOTRegressor #conda install -c conda-forge tpot\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "pipeline_optimizer = TPOTRegressor(generations=5, population_size=10, cv=5,\n",
    "                                    random_state=42, verbosity=2, scoring='neg_mean_absolute_error')\n",
    "pipeline_optimizer.fit(X_train, y_train)\n",
    "print(pipeline_optimizer.score(X_test, y_test))\n",
    "pipeline_optimizer.export('tpot_exported_pipeline.py')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ccb08227af5401e393d69011b51c5f3d765e0becceef24ede2104541ef794f2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
