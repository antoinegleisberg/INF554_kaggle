{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models for non_text features\n",
    "\n",
    "Try automl library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.7, test_size=0.3)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mentions is always empty -> DROP Mention\n",
    "\n",
    "X_train = X_train.drop(['mentions'], axis=1)\n",
    "X_test = X_test.drop(['mentions'], axis=1)\n",
    "\n",
    "#Tweet id is not relevant -> DROP Tweet id\n",
    "X_train = X_train.drop(['TweetID'], axis=1)\n",
    "X_test = X_test.drop(['TweetID'], axis=1)\n",
    "\n",
    "\n",
    "#add a column to data which counts url\n",
    "X_train['url_count'] = X_train['urls'].str.count('http')\n",
    "X_test['url_count'] = X_test['urls'].str.count('http')\n",
    "X_train = X_train.drop(['urls'], axis=1)\n",
    "X_test = X_test.drop(['urls'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#drop the text column\n",
    "X_train = X_train.drop(['text'], axis=1)\n",
    "X_test = X_test.drop(['text'], axis=1)\n",
    "\n",
    "#drop the hashtags column\n",
    "X_train = X_train.drop(['hashtags'], axis=1)\n",
    "X_test = X_test.drop(['hashtags'], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: -10.538812 (0.886428)\n",
      "ScaledLASSO: -10.124824 (0.953406)\n",
      "ScaledEN: -13.899756 (1.088446)\n",
      "ScaledKNN: -7.218124 (0.799233)\n",
      "ScaledCART: -8.597397 (0.835062)\n",
      "ScaledGBM: -6.802502 (0.651244)\n",
      "ScaledRF: -6.391916 (0.703740)\n",
      "ScaledMLP: -8.864348 (0.715103)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    " \n",
    "\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "pipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor(n_estimators=10))])))\n",
    "pipelines.append(('ScaledMLP', Pipeline([('Scaler', StandardScaler()),('MLP', MLPRegressor())])))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 55min: StandardScaler, no extraction, no selection\n",
    "- ScaledLR: -10.538812 (0.886428)\n",
    "- ScaledLASSO: -10.124824 (0.953406)\n",
    "- ScaledEN: -13.899756 (1.088446)\n",
    "- ScaledKNN: -7.218124 (0.799233)\n",
    "- ScaledCART: -8.597397 (0.835062)\n",
    "- ScaledGBM: -6.802502 (0.651244)\n",
    "- ScaledRF: -6.391916 (0.703740)\n",
    "- ScaledMLP: -8.864348 (0.715103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: -10.433137 (0.871373)\n",
      "ScaledLASSO: -20.037637 (1.619121)\n",
      "ScaledEN: -26.620310 (1.340733)\n",
      "ScaledKNN: -7.534165 (0.766204)\n",
      "ScaledCART: -8.391123 (0.749394)\n",
      "ScaledGBM: -6.639914 (0.724651)\n",
      "ScaledRF: -6.230696 (0.714884)\n",
      "ScaledMLP: -8.770942 (0.776259)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    " \n",
    "\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', MinMaxScaler()),('LR',LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', MinMaxScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', MinMaxScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', MinMaxScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', MinMaxScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('Scaler', MinMaxScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "pipelines.append(('ScaledRF', Pipeline([('Scaler', MinMaxScaler()),('RF', RandomForestRegressor(n_estimators=10))])))\n",
    "pipelines.append(('ScaledMLP', Pipeline([('Scaler', MinMaxScaler()),('MLP', MLPRegressor())])))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " No extraction nor selection, MinMaxScaler\n",
    "\n",
    "- ScaledLR: -10.433137 (0.871373)\n",
    "- ScaledLASSO: -20.037637 (1.619121)\n",
    "- ScaledEN: -26.620310 (1.340733)\n",
    "- ScaledKNN: -7.534165 (0.766204)\n",
    "- ScaledCART: -8.391123 (0.749394)\n",
    "- ScaledGBM: -6.639914 (0.724651)\n",
    "- ScaledRF: -6.230696 (0.714884)\n",
    "- ScaledMLP: -8.770942 (0.776259)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bilan: Min MaxScaler is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
