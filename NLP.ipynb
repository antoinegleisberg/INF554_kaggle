{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This include all NLP approaches for word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of Likes and Retweets Using Text Information Retrieval\n",
    "https://ai.intelligentonlinetools.com/ml/text-clustering-doc2vec-word-embedding-machine-learning/\n",
    "\n",
    "Paper:\n",
    "1-s2.0-S1877050920304129-main.pdf\n",
    "\n",
    "\n",
    "Le github magique:\n",
    "https://github.com/buomsoo-kim/Word-embedding-with-Python\n",
    "\n",
    "French corpus: https://stackoverflow.com/questions/42058396/python-nltk-and-textblob-in-french"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding à la main avec Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from scipy import spatial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              David Hume “Essai sur la liberté de la presse”'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#télcharge \"quinze essais politiques\" dans data et en fait une liste\n",
    "# opening the file in read mode\n",
    "my_file = open(\"data/quinze_essais_politiques.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "corpus = my_file.read()\n",
    "#remove \\n\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('.', ',')\n",
    "\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "corpus = corpus.split(\",\")\n",
    "my_file.close()\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use re module to preprocess data\n",
    "\n",
    "Convert all letters into lowercase\n",
    "\n",
    "Remove punctuations, numbers, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['david', 'hume', 'essai', 'sur', 'la', 'liberté', 'de', 'la', 'presse']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    #remove punctuation\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]','',corpus[i])\n",
    "    #make a list of corpus[i]\n",
    "    corpus[i] = corpus[i].split()\n",
    "    \n",
    "\n",
    "corpus[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the doc2vec model, input data should be in format of iterable TaggedDocuments\"\n",
    "\n",
    "Each TaggedDocument instance comprises words and tags\n",
    "\n",
    "Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is identifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['david', 'hume', 'essai', 'sur', 'la', 'liberté', 'de', 'la', 'presse'], tags=['sent5'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = TaggedDocument(words = corpus[i], tags = ['sent{}'.format(i)])    # converting each sentence into a TaggedDocument\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents = corpus, vector_size = 100, min_count = 1)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model.save('doc2vec_model')\n",
    "model = Doc2Vec.load('doc2vec_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04253318905830383"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.infer_vector(['Macron',' démission'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector(['gilets jaune'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "cosine_similarity(v1, v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.77224708e+00, -8.20644140e-01, -3.03102303e+00,  1.71295309e+00,\n",
       "       -1.51811123e-01, -6.19174361e-01,  8.30909967e-01,  3.04782152e+00,\n",
       "        3.13939261e+00, -3.14197016e+00,  3.75156593e+00,  7.06376970e-01,\n",
       "        4.97845650e-01, -7.39134669e-01, -1.51396203e+00, -6.00879252e-01,\n",
       "        1.98837304e+00, -1.65502715e+00, -2.15656042e-01, -3.06843376e+00,\n",
       "       -2.57701969e+00, -6.50485158e-02, -1.93754995e+00, -1.44916689e+00,\n",
       "       -1.40775919e+00, -3.76376247e+00,  7.35826492e-02,  5.47314596e+00,\n",
       "        2.39667892e+00, -1.73095465e-02, -3.04729295e+00,  3.57966995e+00,\n",
       "       -4.89883900e-01,  1.82727182e+00, -7.40789533e-01, -2.31122684e+00,\n",
       "       -6.91391945e-01, -1.97245240e+00,  6.30308867e-01, -3.27573270e-01,\n",
       "       -3.56692076e-01, -8.23963046e-01,  3.11563540e+00, -4.16857243e+00,\n",
       "       -1.66391611e+00, -2.21248603e+00,  2.82474899e+00,  5.70967078e-01,\n",
       "        5.85822582e-01,  2.84727335e+00,  4.80433702e-01, -8.01777303e-01,\n",
       "       -3.93913603e+00, -1.78017902e+00, -2.18786657e-01,  1.74005032e+00,\n",
       "       -3.50419343e-01,  2.97379494e-03, -1.41924715e+00, -1.09670210e+00,\n",
       "       -6.57234609e-01, -1.78825092e+00,  1.54941726e+00,  5.09492254e+00,\n",
       "        1.15031660e+00, -7.18269014e+00, -2.33222675e+00, -1.25129104e-01,\n",
       "        3.04164648e-01, -8.50841999e-01,  4.15074301e+00,  3.08311510e+00,\n",
       "       -8.42067778e-01, -4.88233566e+00,  1.36789572e+00,  4.02609873e+00,\n",
       "        5.34399366e+00, -1.64721310e+00, -2.05765343e+00,  4.76427460e+00,\n",
       "        7.45801449e-01, -1.18876278e+00, -8.82061303e-01,  1.66664743e+00,\n",
       "        8.01591277e-02,  1.96971488e+00,  1.55170977e-01,  6.72619164e-01,\n",
       "        1.22535944e-01,  2.88762540e-01, -3.45716429e+00,  2.76721811e+00,\n",
       "        1.97181845e+00, -7.30734944e-01,  3.91164482e-01, -1.14893556e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# process a sentence using the model\n",
    "doc = nlp(\"Macron démission\")\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "# Get the vector for 'text':\n",
    "doc.vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.7, test_size=0.3)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Spacy text embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting vectoring X_train\n",
      "starting vectoring X_test\n"
     ]
    }
   ],
   "source": [
    "#use nlp model fro text train data\n",
    "print(\"starting vectoring X_train\")\n",
    "\n",
    "X_train_spa = X_train['text'].apply(lambda x: nlp(x).vector)\n",
    "#use nlp model fro text eval data\n",
    "print(\"starting vectoring X_test\")\n",
    "X_test_spa = X_test['text'].apply(lambda x: nlp(x).vector)\n",
    "\n",
    "#expected duration: 30min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_txt to pd\n",
    "X_test_spa = pd.DataFrame(X_test_spa.to_list())\n",
    "#X_train_txt to pd\n",
    "X_train_spa = pd.DataFrame(X_train_spa.to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PCA to reduce dimension\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "result = pca.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projet  INF554\\INF554_kaggle\\NLP.ipynb Cellule 24\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X26sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mstarting model training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X26sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m reg2 \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m30\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X26sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m reg2\u001b[39m.\u001b[39;49mfit(X_train_spa, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X26sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Predict the number of retweets for the evaluation dataset\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X26sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m y_pred_spa \u001b[39m=\u001b[39m reg2\u001b[39m.\u001b[39mpredict(X_test_spa)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:442\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    431\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    432\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    433\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    434\u001b[0m ]\n\u001b[0;32m    436\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    437\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    438\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    443\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    444\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    445\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m_joblib_parallel_args(prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m    446\u001b[0m )(\n\u001b[0;32m    447\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    448\u001b[0m         t,\n\u001b[0;32m    449\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[0;32m    450\u001b[0m         X,\n\u001b[0;32m    451\u001b[0m         y,\n\u001b[0;32m    452\u001b[0m         sample_weight,\n\u001b[0;32m    453\u001b[0m         i,\n\u001b[0;32m    454\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    455\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    456\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    457\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    458\u001b[0m     )\n\u001b[0;32m    459\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    460\u001b[0m )\n\u001b[0;32m    462\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1043\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1034\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1035\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1040\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1041\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1046\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    862\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    780\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:211\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    210\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 211\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:185\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[39melif\u001b[39;00m class_weight \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbalanced_subsample\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    183\u001b[0m         curr_sample_weight \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m compute_sample_weight(\u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, y, indices\u001b[39m=\u001b[39mindices)\n\u001b[1;32m--> 185\u001b[0m     tree\u001b[39m.\u001b[39;49mfit(X, y, sample_weight\u001b[39m=\u001b[39;49mcurr_sample_weight, check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     tree\u001b[39m.\u001b[39mfit(X, y, sample_weight\u001b[39m=\u001b[39msample_weight, check_input\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:1315\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\n\u001b[0;32m   1279\u001b[0m     \u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, check_input\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, X_idx_sorted\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdeprecated\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1280\u001b[0m ):\n\u001b[0;32m   1281\u001b[0m     \u001b[39m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \n\u001b[0;32m   1283\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[39m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1313\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1315\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m   1316\u001b[0m         X,\n\u001b[0;32m   1317\u001b[0m         y,\n\u001b[0;32m   1318\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1319\u001b[0m         check_input\u001b[39m=\u001b[39;49mcheck_input,\n\u001b[0;32m   1320\u001b[0m         X_idx_sorted\u001b[39m=\u001b[39;49mX_idx_sorted,\n\u001b[0;32m   1321\u001b[0m     )\n\u001b[0;32m   1322\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py:420\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    410\u001b[0m     builder \u001b[39m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    411\u001b[0m         splitter,\n\u001b[0;32m    412\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    418\u001b[0m     )\n\u001b[1;32m--> 420\u001b[0m builder\u001b[39m.\u001b[39;49mbuild(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtree_, X, y, sample_weight)\n\u001b[0;32m    422\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m is_classifier(\u001b[39mself\u001b[39m):\n\u001b[0;32m    423\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_classes_[\u001b[39m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor# We fit our model using the training data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"starting model training\")\n",
    "reg3 = GradientBoostingRegressor()\n",
    "reg3.fit(X_train_spa,y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred_spa = reg3.predict(X_test_spa)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred_spa = [int(value) if value >= 0 else 0 for value in y_pred_spa]\n",
    "print(\"Text GB spacy\", mean_absolute_error(y_true=y_test, y_pred=y_pred_spa))\n",
    "\n",
    "\n",
    "print(\"starting model training\")\n",
    "reg1 = MLPRegressor(solver='adam', activation='relu', hidden_layer_sizes=(30,30))\n",
    "reg1.fit(X_train_spa, y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred_spa = reg1.predict(X_test_spa)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred_spa = [int(value) if value >= 0 else 0 for value in y_pred_spa]\n",
    "print(\"Text MLP spacy\", mean_absolute_error(y_true=y_test, y_pred=y_pred_spa))\n",
    "\n",
    "print(\"starting model training\")\n",
    "reg2 = RandomForestRegressor(n_estimators=30)\n",
    "reg2.fit(X_train_spa, y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred_spa = reg2.predict(X_test_spa)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred_spa = [int(value) if value >= 0 else 0 for value in y_pred_spa]\n",
    "print(\"Text RF spacy\", mean_absolute_error(y_true=y_test, y_pred=y_pred_spa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with itdf vectorizer word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model training\n",
      "Text GB tfidf 26.129078735486058\n",
      "starting model training\n",
      "Text RF tfidf 26.433558399487715\n",
      "starting model training\n",
      "Text MLP tfidf 27.056888060193426\n"
     ]
    }
   ],
   "source": [
    "# We set up an Tfidf Vectorizer that will use the top 100 tokens from the tweets. We also remove stopwords.\n",
    "# To do that we have to fit our training dataset and then transform both the training and testing dataset. \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words=stopwords.words('french'))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['text'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['text'])\n",
    "\n",
    "# We fit our model using the training data\n",
    "\n",
    "\n",
    "\n",
    "print(\"starting model training\")\n",
    "reg3= GradientBoostingRegressor()\n",
    "reg3.fit(X_train_tfidf, y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred_tfidf = reg3.predict(X_test_tfidf)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred_tfidf = [int(value) if value >= 0 else 0 for value in y_pred_tfidf]\n",
    "\n",
    "print(\"Text GB tfidf\", mean_absolute_error(y_true=y_test, y_pred=y_pred_tfidf))\n",
    "\n",
    "print(\"starting model training\")\n",
    "reg2 = RandomForestRegressor(n_estimators=30)\n",
    "reg2.fit(X_train_tfidf, y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred_tfidf = reg2.predict(X_test_tfidf)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred_tfidf = [int(value) if value >= 0 else 0 for value in y_pred_tfidf]\n",
    "\n",
    "print(\"Text RF tfidf\", mean_absolute_error(y_true=y_test, y_pred=y_pred_tfidf))\n",
    "\n",
    "print(\"starting model training\")\n",
    "reg1 = MLPRegressor(solver='adam', activation='relu', hidden_layer_sizes=(30,30))\n",
    "#reg = RandomForestRegressor(n_estimators=10, max_depth=10, random_state=0)\n",
    "#reg= GradientBoostingRegressor()\n",
    "reg1.fit(X_train_tfidf, y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred_tfidf = reg1.predict(X_test_tfidf)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred_tfidf = [int(value) if value >= 0 else 0 for value in y_pred_tfidf]\n",
    "\n",
    "print(\"Text MLP tfidf\", mean_absolute_error(y_true=y_test, y_pred=y_pred_tfidf))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text GB tfidf 26.129078735486058\n",
    "\n",
    "Text RF tfidf 26.433558399487715\n",
    "\n",
    "Text MLP tfidf 27.056888060193426"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traiement des Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectorizer2 = TfidfVectorizer(max_features=100, stop_words=stopwords.words('french'))\n",
    "#transform list in hashtags into a string\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
