{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This include all NLP approaches for word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of Likes and Retweets Using Text Information Retrieval\n",
    "https://ai.intelligentonlinetools.com/ml/text-clustering-doc2vec-word-embedding-machine-learning/\n",
    "\n",
    "Paper:\n",
    "1-s2.0-S1877050920304129-main.pdf\n",
    "\n",
    "\n",
    "Le github magique:\n",
    "https://github.com/buomsoo-kim/Word-embedding-with-Python\n",
    "\n",
    "French corpus: https://stackoverflow.com/questions/42058396/python-nltk-and-textblob-in-french"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding à la main avec Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from scipy import spatial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              David Hume “Essai sur la liberté de la presse”'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#télcharge \"quinze essais politiques\" dans data et en fait une liste\n",
    "# opening the file in read mode\n",
    "my_file = open(\"data/quinze_essais_politiques.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "corpus = my_file.read()\n",
    "#remove \\n\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('.', ',')\n",
    "\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "corpus = corpus.split(\",\")\n",
    "my_file.close()\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use re module to preprocess data\n",
    "\n",
    "Convert all letters into lowercase\n",
    "\n",
    "Remove punctuations, numbers, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['david', 'hume', 'essai', 'sur', 'la', 'liberté', 'de', 'la', 'presse']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    #remove punctuation\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]','',corpus[i])\n",
    "    #make a list of corpus[i]\n",
    "    corpus[i] = corpus[i].split()\n",
    "    \n",
    "\n",
    "corpus[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the doc2vec model, input data should be in format of iterable TaggedDocuments\"\n",
    "\n",
    "Each TaggedDocument instance comprises words and tags\n",
    "\n",
    "Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is identifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['david', 'hume', 'essai', 'sur', 'la', 'liberté', 'de', 'la', 'presse'], tags=['sent5'])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = TaggedDocument(words = corpus[i], tags = ['sent{}'.format(i)])    # converting each sentence into a TaggedDocument\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents = corpus, vector_size = 100, min_count = 1)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model.save('doc2vec_model')\n",
    "model = Doc2Vec.load('doc2vec_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14602068066596985"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.infer_vector(['Macron',' démission'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector(['gilets jaune'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "cosine_similarity(v1, v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.77224708e+00, -8.20644140e-01, -3.03102303e+00,  1.71295309e+00,\n",
       "       -1.51811123e-01, -6.19174361e-01,  8.30909967e-01,  3.04782152e+00,\n",
       "        3.13939261e+00, -3.14197016e+00,  3.75156593e+00,  7.06376970e-01,\n",
       "        4.97845650e-01, -7.39134669e-01, -1.51396203e+00, -6.00879252e-01,\n",
       "        1.98837304e+00, -1.65502715e+00, -2.15656042e-01, -3.06843376e+00,\n",
       "       -2.57701969e+00, -6.50485158e-02, -1.93754995e+00, -1.44916689e+00,\n",
       "       -1.40775919e+00, -3.76376247e+00,  7.35826492e-02,  5.47314596e+00,\n",
       "        2.39667892e+00, -1.73095465e-02, -3.04729295e+00,  3.57966995e+00,\n",
       "       -4.89883900e-01,  1.82727182e+00, -7.40789533e-01, -2.31122684e+00,\n",
       "       -6.91391945e-01, -1.97245240e+00,  6.30308867e-01, -3.27573270e-01,\n",
       "       -3.56692076e-01, -8.23963046e-01,  3.11563540e+00, -4.16857243e+00,\n",
       "       -1.66391611e+00, -2.21248603e+00,  2.82474899e+00,  5.70967078e-01,\n",
       "        5.85822582e-01,  2.84727335e+00,  4.80433702e-01, -8.01777303e-01,\n",
       "       -3.93913603e+00, -1.78017902e+00, -2.18786657e-01,  1.74005032e+00,\n",
       "       -3.50419343e-01,  2.97379494e-03, -1.41924715e+00, -1.09670210e+00,\n",
       "       -6.57234609e-01, -1.78825092e+00,  1.54941726e+00,  5.09492254e+00,\n",
       "        1.15031660e+00, -7.18269014e+00, -2.33222675e+00, -1.25129104e-01,\n",
       "        3.04164648e-01, -8.50841999e-01,  4.15074301e+00,  3.08311510e+00,\n",
       "       -8.42067778e-01, -4.88233566e+00,  1.36789572e+00,  4.02609873e+00,\n",
       "        5.34399366e+00, -1.64721310e+00, -2.05765343e+00,  4.76427460e+00,\n",
       "        7.45801449e-01, -1.18876278e+00, -8.82061303e-01,  1.66664743e+00,\n",
       "        8.01591277e-02,  1.96971488e+00,  1.55170977e-01,  6.72619164e-01,\n",
       "        1.22535944e-01,  2.88762540e-01, -3.45716429e+00,  2.76721811e+00,\n",
       "        1.97181845e+00, -7.30734944e-01,  3.91164482e-01, -1.14893556e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# process a sentence using the model\n",
    "doc = nlp(\"Macron démission\")\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "# Get the vector for 'text':\n",
    "doc.vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Spacy text embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.7, test_size=0.3)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RamdomForestRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projet  INF554\\INF554_kaggle\\NLP.ipynb Cellule 19\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_test_txt \u001b[39m=\u001b[39m X_test[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: nlp(x)\u001b[39m.\u001b[39mvector)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# We fit our model using the training data\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m reg \u001b[39m=\u001b[39m RamdomForestRegressor()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m reg\u001b[39m.\u001b[39mfit(X_train_txt, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X23sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Predict the number of retweets for the evaluation dataset\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'RamdomForestRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "#use nlp model fro text train data\n",
    "X_train_txt = X_train['text'].apply(lambda x: nlp(x).vector)\n",
    "#use nlp model fro text eval data\n",
    "print(\"starting vecoring X_test\")\n",
    "X_test_txt = X_test['text'].apply(lambda x: nlp(x).vector)\n",
    "\n",
    "#expected duration: 26min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_txt to pd\n",
    "X_test_txt = pd.DataFrame(X_test_txt.to_list())\n",
    "#X_train_txt to pd\n",
    "X_train_txt = pd.DataFrame(X_train_txt.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model training\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# We fit our model using the training data\n",
    "print(\"starting model training\")\n",
    "reg = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=0)\n",
    "reg.fit(X_train_txt, y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred = reg.predict(X_test_txt)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred = [int(value) if value >= 0 else 0 for value in y_pred]\n",
    "\n",
    "print(\"Text RF\", mean_absolute_error(y_true=y_test, y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
