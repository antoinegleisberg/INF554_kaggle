{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP word embedding-> Final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit # pip install verstack\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This include all NLP approaches for word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of Likes and Retweets Using Text Information Retrieval\n",
    "https://ai.intelligentonlinetools.com/ml/text-clustering-doc2vec-word-embedding-machine-learning/\n",
    "\n",
    "Paper:\n",
    "1-s2.0-S1877050920304129-main.pdf\n",
    "\n",
    "\n",
    "Le github magique:\n",
    "https://github.com/buomsoo-kim/Word-embedding-with-Python\n",
    "\n",
    "French corpus: https://stackoverflow.com/questions/42058396/python-nltk-and-textblob-in-french"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding à la main avec Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from scipy import spatial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              David Hume “Essai sur la liberté de la presse”'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#télcharge \"quinze essais politiques\" dans data et en fait une liste\n",
    "# opening the file in read mode\n",
    "my_file = open(\"data/quinze_essais_politiques.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "corpus = my_file.read()\n",
    "#remove \\n\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('.', ',')\n",
    "\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "corpus = corpus.split(\",\")\n",
    "my_file.close()\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use re module to preprocess data\n",
    "\n",
    "Convert all letters into lowercase\n",
    "\n",
    "Remove punctuations, numbers, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['david', 'hume', 'essai', 'sur', 'la', 'liberté', 'de', 'la', 'presse']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    #remove punctuation\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]','',corpus[i])\n",
    "    #make a list of corpus[i]\n",
    "    corpus[i] = corpus[i].split()\n",
    "    \n",
    "\n",
    "corpus[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the doc2vec model, input data should be in format of iterable TaggedDocuments\"\n",
    "\n",
    "Each TaggedDocument instance comprises words and tags\n",
    "\n",
    "Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is identifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['david', 'hume', 'essai', 'sur', 'la', 'liberté', 'de', 'la', 'presse'], tags=['sent5'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = TaggedDocument(words = corpus[i], tags = ['sent{}'.format(i)])    # converting each sentence into a TaggedDocument\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(documents = corpus, vector_size = 100, min_count = 1)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model.save('doc2vec_model')\n",
    "model = Doc2Vec.load('doc2vec_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04253318905830383"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = model.infer_vector(['Macron',' démission'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector(['gilets jaune'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "cosine_similarity(v1, v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.77224708e+00, -8.20644140e-01, -3.03102303e+00,  1.71295309e+00,\n",
       "       -1.51811123e-01, -6.19174361e-01,  8.30909967e-01,  3.04782152e+00,\n",
       "        3.13939261e+00, -3.14197016e+00,  3.75156593e+00,  7.06376970e-01,\n",
       "        4.97845650e-01, -7.39134669e-01, -1.51396203e+00, -6.00879252e-01,\n",
       "        1.98837304e+00, -1.65502715e+00, -2.15656042e-01, -3.06843376e+00,\n",
       "       -2.57701969e+00, -6.50485158e-02, -1.93754995e+00, -1.44916689e+00,\n",
       "       -1.40775919e+00, -3.76376247e+00,  7.35826492e-02,  5.47314596e+00,\n",
       "        2.39667892e+00, -1.73095465e-02, -3.04729295e+00,  3.57966995e+00,\n",
       "       -4.89883900e-01,  1.82727182e+00, -7.40789533e-01, -2.31122684e+00,\n",
       "       -6.91391945e-01, -1.97245240e+00,  6.30308867e-01, -3.27573270e-01,\n",
       "       -3.56692076e-01, -8.23963046e-01,  3.11563540e+00, -4.16857243e+00,\n",
       "       -1.66391611e+00, -2.21248603e+00,  2.82474899e+00,  5.70967078e-01,\n",
       "        5.85822582e-01,  2.84727335e+00,  4.80433702e-01, -8.01777303e-01,\n",
       "       -3.93913603e+00, -1.78017902e+00, -2.18786657e-01,  1.74005032e+00,\n",
       "       -3.50419343e-01,  2.97379494e-03, -1.41924715e+00, -1.09670210e+00,\n",
       "       -6.57234609e-01, -1.78825092e+00,  1.54941726e+00,  5.09492254e+00,\n",
       "        1.15031660e+00, -7.18269014e+00, -2.33222675e+00, -1.25129104e-01,\n",
       "        3.04164648e-01, -8.50841999e-01,  4.15074301e+00,  3.08311510e+00,\n",
       "       -8.42067778e-01, -4.88233566e+00,  1.36789572e+00,  4.02609873e+00,\n",
       "        5.34399366e+00, -1.64721310e+00, -2.05765343e+00,  4.76427460e+00,\n",
       "        7.45801449e-01, -1.18876278e+00, -8.82061303e-01,  1.66664743e+00,\n",
       "        8.01591277e-02,  1.96971488e+00,  1.55170977e-01,  6.72619164e-01,\n",
       "        1.22535944e-01,  2.88762540e-01, -3.45716429e+00,  2.76721811e+00,\n",
       "        1.97181845e+00, -7.30734944e-01,  3.91164482e-01, -1.14893556e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('fr_core_news_sm')# python -m spacy download fr_core_news_sm\n",
    "# process a sentence using the model\n",
    "doc = nlp(\"Macron démission\")\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "# Get the vector for 'text':\n",
    "doc.vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.7, test_size=0.3)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Spacy text embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting vectoring X_train\n",
      "starting vectoring X_test\n"
     ]
    }
   ],
   "source": [
    "#use nlp model fro text train data\n",
    "print(\"starting vectoring X_train\")\n",
    "\n",
    "X_train_spa = X_train['text'].apply(lambda x: nlp(x).vector)\n",
    "#use nlp model fro text eval data\n",
    "print(\"starting vectoring X_test\")\n",
    "X_test_spa = X_test['text'].apply(lambda x: nlp(x).vector)\n",
    "\n",
    "#expected duration: 30min\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_txt to pd\n",
    "X_test_spa = pd.DataFrame(X_test_spa.to_list())\n",
    "#X_train_txt to pd\n",
    "X_train_spa = pd.DataFrame(X_train_spa.to_list())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### PCA to reduce dimension\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=10)\n",
    "X_train_spa_pca = pca.fit_transform(X_train_spa)\n",
    "X_test_spa_pca = pca.transform(X_test_spa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: -26.814401 (1.000995)\n",
      "ScaledLASSO: -26.780504 (1.006401)\n",
      "ScaledEN: -26.780629 (1.003824)\n",
      "ScaledKNN: -29.531135 (1.171399)\n",
      "ScaledCART: -32.986936 (1.122446)\n",
      "ScaledGBM: -26.696848 (1.031312)\n",
      "ScaledRF: -30.585169 (0.964902)\n",
      "ScaledMLP: -27.521220 (0.938561)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "pipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\n",
    "pipelines.append(('ScaledMLP', Pipeline([('Scaler', StandardScaler()),('MLP', MLPRegressor())])))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results = cross_val_score(model, X_train_spa_pca, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sans PCA (53min):\n",
    "\n",
    "- Text GB spacy 26.827508922601726\n",
    "- Text MLP spacy 30.977323878671452\n",
    "- Text RF spacy 35.9382998559199\n",
    "\n",
    "PCA=20components (11min):\n",
    "\n",
    "- Text GB spacy 26.841869838310213\n",
    "- Text MLP spacy 26.872974169185714\n",
    "- Text RF spacy 32.52075976306843\n",
    "\n",
    "PCA=10 components (126min):\n",
    "- ScaledLR: -26.814401 (1.000995)\n",
    "- ScaledLASSO: -26.780504 (1.006401)\n",
    "- ScaledEN: -26.780629 (1.003824)\n",
    "- ScaledKNN: -29.531135 (1.171399)\n",
    "- ScaledCART: -32.986936 (1.122446)\n",
    "- ScaledGBM: -26.696848 (1.031312)\n",
    "- ScaledRF: -30.585169 (0.964902)\n",
    "- ScaledMLP: -27.521220 (0.938561)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: -26.814388 (1.000974)\n",
      "ScaledLASSO: -26.878887 (1.000569)\n",
      "ScaledEN: -26.878887 (1.000569)\n",
      "ScaledKNN: -29.499869 (1.200428)\n",
      "ScaledCART: -33.103061 (1.433910)\n",
      "ScaledGBM: -26.720753 (1.023079)\n",
      "ScaledRF: -31.792145 (0.712734)\n",
      "ScaledMLP: -26.933599 (1.345101)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "pipelines=[]\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', MinMaxScaler()),('LR',LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', MinMaxScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', MinMaxScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', MinMaxScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', MinMaxScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('Scaler', MinMaxScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "pipelines.append(('ScaledRF', Pipeline([('Scaler', MinMaxScaler()),('RF', RandomForestRegressor(n_estimators=10))])))\n",
    "pipelines.append(('ScaledMLP', Pipeline([('Scaler', MinMaxScaler()),('MLP', MLPRegressor())])))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results = cross_val_score(model, X_train_spa_pca, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with itdf vectorizer word embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set up an Tfidf Vectorizer that will use the top 100 tokens from the tweets. We also remove stopwords.\n",
    "# To do that we have to fit our training dataset and then transform both the training and testing dataset. \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=100, stop_words=stopwords.words('french'))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train['text'])\n",
    "X_test_tfidf = vectorizer.transform(X_test['text'])\n",
    "#Xtrain_tfidf to data frame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledLR: nan (nan)\n",
      "ScaledLASSO: nan (nan)\n",
      "ScaledEN: nan (nan)\n",
      "ScaledKNN: nan (nan)\n",
      "ScaledCART: nan (nan)\n",
      "ScaledGBM: nan (nan)\n",
      "ScaledRF: nan (nan)\n",
      "ScaledMLP: nan (nan)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\n",
    "pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "pipelines.append(('ScaledEN', Pipeline([('Scaler', StandardScaler()),('EN', ElasticNet())])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "pipelines.append(('ScaledRF', Pipeline([('Scaler', StandardScaler()),('RF', RandomForestRegressor())])))\n",
    "pipelines.append(('ScaledMLP', Pipeline([('Scaler', StandardScaler()),('MLP', MLPRegressor())])))\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=10)\n",
    "    cv_results = cross_val_score(model, X_train_tfidf, y_train, cv=kfold, scoring='neg_mean_absolute_error')\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text GB tfidf 26.129078735486058\n",
    "\n",
    "Text RF tfidf 26.433558399487715\n",
    "\n",
    "Text MLP tfidf 27.056888060193426"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traiement des Hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "vectorizer2 = TfidfVectorizer(max_features=100, stop_words=stopwords.words('french'))\n",
    "#transform list in hashtags into a string\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
