{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This include all NLP approaches for word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction of Likes and Retweets Using Text Information Retrieval\n",
    "https://ai.intelligentonlinetools.com/ml/text-clustering-doc2vec-word-embedding-machine-learning/\n",
    "\n",
    "Paper:\n",
    "1-s2.0-S1877050920304129-main.pdf\n",
    "\n",
    "\n",
    "Le github magique:\n",
    "https://github.com/buomsoo-kim/Word-embedding-with-Python\n",
    "\n",
    "French corpus: https://stackoverflow.com/questions/42058396/python-nltk-and-textblob-in-french"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding à la main avec Doc 2 Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import gutenberg\n",
    "from multiprocessing import Pool\n",
    "from scipy import spatial\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              David Hume “Essai sur la liberté de la presse”'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#télcharge \"quinze essais politiques\" dans data et en fait une liste\n",
    "# opening the file in read mode\n",
    "my_file = open(\"data/quinze_essais_politiques.txt\", \"r\")\n",
    "  \n",
    "# reading the file\n",
    "corpus = my_file.read()\n",
    "#remove \\n\n",
    "corpus = corpus.replace('\\n', ' ')\n",
    "corpus = corpus.replace('.', ',')\n",
    "\n",
    "  \n",
    "# replacing end splitting the text \n",
    "# when newline ('\\n') is seen.\n",
    "corpus = corpus.split(\",\")\n",
    "my_file.close()\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use re module to preprocess data\n",
    "\n",
    "Convert all letters into lowercase\n",
    "\n",
    "Remove punctuations, numbers, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = corpus[i].lower()\n",
    "    #remove punctuation\n",
    "    corpus[i] = re.sub(r'[^\\w\\s]','',corpus[i])\n",
    "    #make a list of corpus[i]\n",
    "    corpus[i] = corpus[i].split()\n",
    "    \n",
    "\n",
    "corpus[5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the doc2vec model, input data should be in format of iterable TaggedDocuments\"\n",
    "\n",
    "Each TaggedDocument instance comprises words and tags\n",
    "\n",
    "Hence, each document (i.e., a sentence or paragraph) should have a unique tag which is identifiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TaggedDocument' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projet  INF554\\INF554_kaggle\\NLP.ipynb Cellule 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(corpus)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     corpus[i] \u001b[39m=\u001b[39m TaggedDocument(words \u001b[39m=\u001b[39m corpus[i], tags \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39msent\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(i)])    \u001b[39m# converting each sentence into a TaggedDocument\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m corpus[\u001b[39m5\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TaggedDocument' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = TaggedDocument(words = corpus[i], tags = ['sent{}'.format(i)])    # converting each sentence into a TaggedDocument\n",
    "corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Doc2Vec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projet  INF554\\INF554_kaggle\\NLP.ipynb Cellule 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m Doc2Vec(documents \u001b[39m=\u001b[39m corpus, vector_size \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m, min_count \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m model\u001b[39m.\u001b[39minit_sims(replace \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/NLP.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mdoc2vec_model\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Doc2Vec' is not defined"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(documents = corpus, vector_size = 100, min_count = 1)\n",
    "model.init_sims(replace = True)\n",
    "\n",
    "model.save('doc2vec_model')\n",
    "model = Doc2Vec.load('doc2vec_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = model.infer_vector(['Macron',' démission'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "v2 = model.infer_vector(['gilets jaune'])    # in doc2vec, infer_vector() function is used to infer the vector embedding of a document\n",
    "# define a function that computes cosine similarity between two words\n",
    "def cosine_similarity(v1, v2):\n",
    "    return 1 - spatial.distance.cosine(v1, v2)\n",
    "cosine_similarity(v1, v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding with transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# Load the spacy model that you have installed\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "# process a sentence using the model\n",
    "doc = nlp(\"Macron démission\")\n",
    "# It's that simple - all of the vectors and words are assigned after this point\n",
    "# Get the vector for 'text':\n",
    "doc.vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with Spacy text embedding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "\n",
    "\n",
    "# Here we split our training data into trainig and testing set. This way we can estimate the evaluation of our model without uploading to Kaggle and avoid overfitting over our evaluation dataset.\n",
    "# scsplit method is used in order to split our regression data in a stratisfied way and keep a similar distribution of retweet counts between the two sets\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.7, test_size=0.3)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use nlp model fro text train data\n",
    "X_train_txt = X_train['text'].apply(lambda x: nlp(x).vector)\n",
    "#use nlp model fro text eval data\n",
    "X_test_txt = X_test['text'].apply(lambda x: nlp(x).vector)\n",
    "# We fit our model using the training data\n",
    "reg = RamdomForestRegressor()\n",
    "reg.fit(X_train_test, y_train)\n",
    "# Predict the number of retweets for the evaluation dataset\n",
    "y_pred = reg.predict(X_test_txt)\n",
    "# We want to make sure that all predictions are non-negative integers\n",
    "y_pred = [int(value) if value >= 0 else 0 for value in y_pred]\n",
    "\n",
    "print(\"Text RF\", mean_absolute_error(y_true=y_test, y_pred=y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
