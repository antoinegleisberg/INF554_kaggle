{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "Source: https://github.com/haradai1262/CIKM2020-AnalytiCup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "- le nN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sns as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit  # pip install verstack\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "train_data.drop(['mentions'], axis=1)\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "eval_data.drop(['mentions'], axis=1)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = scsplit(\n",
    "    train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.8, test_size=0.2)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrac features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def extract_time_features(df):\n",
    "    rs_df = df\n",
    "    rs_df[\"hour\"] = rs_df['timestamp'].apply(\n",
    "        lambda t: (datetime.fromtimestamp(t//1000).hour))\n",
    "    rs_df[\"day\"] = rs_df['timestamp'].apply(\n",
    "        lambda t: (datetime.fromtimestamp(t//1000)).weekday())\n",
    "    rs_df[\"week_in_month\"] = rs_df['timestamp'].apply(\n",
    "        lambda t: (datetime.fromtimestamp(t//1000).day)//7)  \n",
    "    rs_df=rs_df.drop(['timestamp'], axis=1)\n",
    "    return rs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_ratio_features(df):\n",
    "    rs_df = df\n",
    "    rs_df['followers__favorites'] = rs_df['followers_count'] * rs_df['favorites_count']\n",
    "    rs_df['friends__favorites'] = rs_df['friends_count'] * rs_df['favorites_count']\n",
    "    rs_df['followers__friends__favorites'] = rs_df['followers_count'] * rs_df['friends_count'] * rs_df['favorites_count']\n",
    "    return rs_df\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import zscore\n",
    "def extract_transfo(df,columns):\n",
    "    rs_df = df\n",
    "    for col in columns:\n",
    "        mean = rs_df[col].mean()\n",
    "        std = rs_df[col].std()\n",
    "        rs_df[col+'_cdf'] = norm.cdf(rs_df[col].values, loc=mean, scale=std)\n",
    "        rs_df[col+'_z'] = zscore(rs_df[col].values)       \n",
    "        rs_df[col+'_rank'] = rs_df[col].rank(method='min')\n",
    "        rs_df[col+'log'] = (df[col] + 1).apply(np.log)\n",
    "    return rs_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "def extract_topic(df):\n",
    "    rs_df = df\n",
    "    rs_df['hashtags'] = rs_df['hashtags'].apply(\n",
    "        lambda x: x.replace('[', '').replace(']', '').replace(\"'\", ''))\n",
    "    #join text and hashtags\n",
    "    rs_df['total_text'] = rs_df['text'] + ' ' + rs_df['hashtags']\n",
    "    vectorizer = TfidfVectorizer(min_df=1, max_features=None, stop_words=stopwords.words('french'))\n",
    "    vector = vectorizer.fit_transform(X_train['text'])\n",
    "    svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "    svd.fit(vector)\n",
    "    topic=svd.transform(vector)\n",
    "    rs_df['topic_1'] = topic[:,0]\n",
    "    rs_df['topic_2'] = topic[:,1]\n",
    "    rs_df['topic_3'] = topic[:,2]\n",
    "    rs_df['topic_4'] = topic[:,3]\n",
    "    rs_df['topic_5'] = topic[:,4]\n",
    "    rs_df=rs_df.drop(['hashtags'],axis=1)\n",
    "    rs_df=rs_df.drop(['total_text'],axis=1)\n",
    "    return rs_df\n",
    "\n",
    "from textblob import TextBlob  # pip install textblob-fr\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "\n",
    "def sent_engineering(in_df):\n",
    "    rs_df = in_df\n",
    "    # add columns related to sentiment analysis\n",
    "    rs_df['polarity'] = rs_df['text'].apply(lambda x: TextBlob(\n",
    "        x, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[0])\n",
    "    rs_df['subjectivity'] = rs_df['text'].apply(lambda x: TextBlob(\n",
    "        x, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[1])\n",
    "    # drop the text column\n",
    "    rs_df = rs_df.drop(['text'], axis=1)\n",
    "\n",
    "    return rs_df\n",
    "\n",
    "def extract_url(in_df):\n",
    "    #count url\n",
    "    rs_df = in_df\n",
    "    rs_df['url_count'] = rs_df['urls'].apply(lambda x: len(x.split(',')))\n",
    "    rs_df=rs_df.drop(['urls'],axis=1)\n",
    "\n",
    "    return rs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cluster(df,columns):\n",
    "    rs_df = df\n",
    "    rs_df['cluster'] = KMeans(n_clusters=100, random_state=0).fit_predict(rs_df[columns].values.reshape(-1, 1))\n",
    "    return rs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_metrics_features = [\n",
    "    'followers_count', 'friends_count', 'favorites_count',\n",
    "    'followers__favorites', 'friends__favorites', 'followers__friends__favorites',\n",
    "]\n",
    "\n",
    "tweet_metrics_log_features = [feat+'_log' for feat in tweet_metrics_features]\n",
    "tweet_metrics_cdf_features = [feat+'_cdf' for feat in tweet_metrics_features]\n",
    "tweet_metrics_z_features = [feat+'_z' for feat in tweet_metrics_features]\n",
    "tweet_metrics_rank_features = [feat+'_rank' for feat in tweet_metrics_features]\n",
    "\n",
    "time_cat_features = ['hour', 'day', 'week_of_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_train=extract_time_features(X_train)\n",
    "new_X_train=extract_ratio_features(new_X_train)\n",
    "new_X_train=extract_transfo(new_X_train,tweet_metrics_features)\n",
    "new_X_train=extract_topic(new_X_train)\n",
    "new_X_train=sent_engineering(new_X_train)\n",
    "new_X_train=extract_url(new_X_train)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        favorites_count  followers_count  statuses_count  friends_count  \\\n",
      "319441             1316            18232           66732           2956   \n",
      "148273                0               71            4332            245   \n",
      "35324                10             3606          236301            236   \n",
      "150236                2              632          213431            969   \n",
      "148734               49              461            1769            293   \n",
      "\n",
      "       mentions  verified  TweetID  hour  day  week_in_month  ...  \\\n",
      "319441       []         0   582659    11    2              1  ...   \n",
      "148273       []         0   310171    10    1              2  ...   \n",
      "35324        []         0   231722    19    4              1  ...   \n",
      "150236       []         0   179476    10    5              1  ...   \n",
      "148734       []         0   595839    21    2              1  ...   \n",
      "\n",
      "        followers__friends__favorites_rank  followers__friends__favoriteslog  \\\n",
      "319441                            280691.0                         24.984878   \n",
      "148273                                 1.0                          0.000000   \n",
      "35324                             236720.0                         15.956771   \n",
      "150236                            222709.0                         14.018302   \n",
      "148734                            234757.0                         15.705391   \n",
      "\n",
      "         topic_1   topic_2   topic_3   topic_4   topic_5  polarity  \\\n",
      "319441  0.011033  0.000523  0.000873  0.004042  0.002255    0.0000   \n",
      "148273  0.150518  0.015317  0.086683  0.063777 -0.057808    0.0000   \n",
      "35324   0.100417 -0.052126 -0.047524  0.015474 -0.002038   -0.0875   \n",
      "150236  0.141567  0.010574 -0.021959  0.292674  0.043168    0.1700   \n",
      "148734  0.106649 -0.004607 -0.022317  0.144987  0.044188   -0.0650   \n",
      "\n",
      "        subjectivity  url_count  \n",
      "319441         0.000          1  \n",
      "148273         0.000          1  \n",
      "35324          0.025          1  \n",
      "150236         0.000          1  \n",
      "148734         0.075          1  \n",
      "\n",
      "[5 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "print(new_X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time_num_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projet  INF554\\INF554_kaggle\\DNN.ipynb Cellule 16\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m dense_features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tweet_metrics_z_features\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m dense_features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tweet_metrics_rank_features\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m dense_features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time_num_features\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m dense_features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m count_encording_features\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m dense_features \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m target_encording_features\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time_num_features' is not defined"
     ]
    }
   ],
   "source": [
    "dense_features = []\n",
    "dense_features += tweet_metrics_features\n",
    "dense_features += tweet_metrics_log_features\n",
    "dense_features += tweet_metrics_cdf_features\n",
    "dense_features += tweet_metrics_z_features\n",
    "dense_features += tweet_metrics_rank_features\n",
    "dense_features += time_num_features\n",
    "dense_features += count_encording_features\n",
    "dense_features += target_encording_features\n",
    "dense_features += text_tfidf_features\n",
    "dense_features += varlen_count_encording_features\n",
    "dense_features += varlen_target_encording_features\n",
    "dense_features += user_stats_features\n",
    "\n",
    "sparse_features = []\n",
    "sparse_features += sentiment_features\n",
    "sparse_features += time_cat_features\n",
    "sparse_features += user_clustering_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LogCoshLoss(torch.nn.Module):\n",
    "    '''\n",
    "    ref: https://github.com/tuantle/regression-losses-pytorch/blob/master/regression_losses.py\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, y_t, y_prime_t):\n",
    "        ey_t = y_t - y_prime_t\n",
    "        return torch.mean(torch.log(torch.cosh(ey_t + 1e-12)))\n",
    "\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    '''\n",
    "    ref: https://github.com/shenweichen/DeepCTR-Torch/blob/master/deepctr_torch/layers/core.py\n",
    "    '''\n",
    "    \"\"\"The Multi Layer Percetron\n",
    "      Input shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., input_dim)``. The most common situation would be a 2D input with shape ``(batch_size, input_dim)``.\n",
    "      Output shape\n",
    "        - nD tensor with shape: ``(batch_size, ..., hidden_size[-1])``. For instance, for a 2D input with shape ``(batch_size, input_dim)``, the output would have shape ``(batch_size, hidden_size[-1])``.\n",
    "      Arguments\n",
    "        - **inputs_dim**: input feature dimension.\n",
    "        - **hidden_units**:list of positive integer, the layer number and units in each layer.\n",
    "        - **activation**: Activation function to use.\n",
    "        - **l2_reg**: float between 0 and 1. L2 regularizer strength applied to the kernel weights matrix.\n",
    "        - **dropout_rate**: float in [0,1). Fraction of the units to dropout.\n",
    "        - **use_bn**: bool. Whether use BatchNormalization before activation or not.\n",
    "        - **seed**: A Python integer to use as random seed.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, inputs_dim, hidden_units, activation='relu', l2_reg=0, dropout_rate=0, use_bn=False,\n",
    "                 init_std=0.0001, dice_dim=3, seed=1024, device='cpu'):\n",
    "        super(DNN, self).__init__()\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.seed = seed\n",
    "        self.l2_reg = l2_reg\n",
    "        self.use_bn = use_bn\n",
    "        if len(hidden_units) == 0:\n",
    "            raise ValueError(\"hidden_units is empty!!\")\n",
    "        hidden_units = [inputs_dim] + list(hidden_units)\n",
    "\n",
    "        self.linears = nn.ModuleList(\n",
    "            [nn.Linear(hidden_units[i], hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        if self.use_bn:\n",
    "            self.bn = nn.ModuleList(\n",
    "                [nn.BatchNorm1d(hidden_units[i + 1]) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        self.activation_layers = nn.ModuleList(\n",
    "            [activation_layer(activation, hidden_units[i + 1], dice_dim) for i in range(len(hidden_units) - 1)])\n",
    "\n",
    "        for name, tensor in self.linears.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                nn.init.normal_(tensor, mean=0, std=init_std)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        deep_input = inputs\n",
    "\n",
    "        for i in range(len(self.linears)):\n",
    "\n",
    "            fc = self.linears[i](deep_input)\n",
    "\n",
    "            if self.use_bn:\n",
    "                fc = self.bn[i](fc)\n",
    "\n",
    "            fc = self.activation_layers[i](fc)\n",
    "\n",
    "            fc = self.dropout(fc)\n",
    "            deep_input = fc\n",
    "        return deep_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dnn_input, dnn_hidden_units, dnn_dropout,\n",
    "        activation='relu', use_bn=True, l2_reg=1e-4, init_std=1e-4,\n",
    "        device='cpu',\n",
    "        feature_index={},\n",
    "        embedding_dict={},\n",
    "        dense_features=[],\n",
    "        sparse_features=[],\n",
    "        varlen_sparse_features=[],\n",
    "        varlen_mode_list=[],\n",
    "        embedding_size=8,\n",
    "        batch_size=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.feature_index = feature_index\n",
    "        self.embedding_dict = embedding_dict\n",
    "        self.dense_features = dense_features\n",
    "        self.sparse_features = sparse_features\n",
    "        self.varlen_sparse_features = varlen_sparse_features\n",
    "        self.varlen_mode_list = varlen_mode_list\n",
    "        self.embedding_size = embedding_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.reg_loss = torch.zeros((1,), device=device)\n",
    "\n",
    "        self.dnn = DNN(#######################################USEFUl\n",
    "            dnn_input, dnn_hidden_units,\n",
    "            activation='relu', l2_reg=l2_reg, dropout_rate=dnn_dropout, use_bn=use_bn,\n",
    "            init_std=init_std, device=device\n",
    "        )\n",
    "        self.dnn_linear = nn.Linear(dnn_hidden_units[-1], 1, bias=False).to(device)\n",
    "\n",
    "        # add regularization\n",
    "        self.add_regularization_loss(self.embedding_dict.parameters(), l2_reg)\n",
    "        self.add_regularization_loss(\n",
    "            filter(lambda x: 'weight' in x[0] and 'bn' not in x[0], self.dnn.named_parameters()), l2_reg)\n",
    "        self.add_regularization_loss(self.dnn_linear.weight, l2_reg)\n",
    "\n",
    "        self.out = nn.Sigmoid()\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        dense_value_list = [\n",
    "            X[:, self.feature_index[feat]: self.feature_index[feat] + 1] for feat in self.dense_features\n",
    "        ]\n",
    "        sparse_embedding_list = [\n",
    "            self.embedding_dict[feat](\n",
    "                X[:, self.feature_index[feat]: self.feature_index[feat] + 1].long()\n",
    "            ) for feat in self.sparse_features\n",
    "        ]\n",
    "        varlen_sparse_embedding_list = get_varlen_pooling_list(\n",
    "            self.embedding_dict, X, self.feature_index, self.varlen_sparse_features, self.varlen_mode_list, self.device\n",
    "        )\n",
    "        sparse_embedding_list = sparse_embedding_list + varlen_sparse_embedding_list\n",
    "\n",
    "        sparse_dnn_input = torch.flatten(torch.cat(sparse_embedding_list, dim=-1), start_dim=1)\n",
    "        dense_dnn_input = torch.flatten(torch.cat(dense_value_list, dim=-1), start_dim=1)\n",
    "\n",
    "        dnn_input = torch.cat([sparse_dnn_input, dense_dnn_input], dim=-1)\n",
    "\n",
    "\n",
    "        dnn_output = self.dnn(dnn_input)\n",
    "        \n",
    "        dnn_logit = self.dnn_linear(dnn_output)\n",
    "\n",
    "        y_pred = self.out(dnn_logit)\n",
    "        return y_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
