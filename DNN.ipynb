{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "\n",
    "Source: https://github.com/haradai1262/CIKM2020-AnalytiCup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO\n",
    "- test various combinations of features with 1st NN (current neurons numbers and batch sizes seems to perform better and = those of the paper)\n",
    "- faire le dnn avec pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sns as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from verstack.stratified_continuous_split import scsplit  # pip install verstack\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "train_data = pd.read_csv(\"data/train.csv\")\n",
    "train_data=train_data.drop(['mentions'], axis=1)\n",
    "\n",
    "# Load the evaluation data\n",
    "eval_data = pd.read_csv(\"data/evaluation.csv\")\n",
    "eval_data=eval_data.drop(['mentions'], axis=1)\n",
    "\n",
    "# split data\n",
    "X_train, X_test, y_train, y_test = scsplit(train_data, train_data['retweets_count'], stratify=train_data['retweets_count'], train_size=0.8, test_size=0.2)\n",
    "# We remove the actual number of retweets from our features since it is the value that we are trying to predict\n",
    "X_train = X_train.drop(['retweets_count'], axis=1)\n",
    "X_test = X_test.drop(['retweets_count'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrac features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extractions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def extract_time_features(df):\n",
    "    rs_df = df\n",
    "    rs_df[\"hour\"] = rs_df['timestamp'].apply(\n",
    "        lambda t: (datetime.fromtimestamp(t//1000).hour))\n",
    "    rs_df[\"day\"] = rs_df['timestamp'].apply(\n",
    "        lambda t: (datetime.fromtimestamp(t//1000)).weekday())\n",
    "    rs_df[\"week_in_month\"] = rs_df['timestamp'].apply(\n",
    "        lambda t: (datetime.fromtimestamp(t//1000).day)//7)  \n",
    "    rs_df=rs_df.drop(['timestamp'], axis=1)\n",
    "    return rs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_ratio_features(df):\n",
    "    rs_df = df\n",
    "    rs_df['followers__favorites'] = rs_df['followers_count'] * rs_df['favorites_count']\n",
    "    rs_df['friends__favorites'] = rs_df['friends_count'] * rs_df['favorites_count']\n",
    "    rs_df['followers__friends__favorites'] = rs_df['followers_count'] * rs_df['friends_count'] * rs_df['favorites_count']\n",
    "    return rs_df\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import zscore\n",
    "def extract_transfo(df,columns):\n",
    "    rs_df = df\n",
    "    for col in columns:\n",
    "        mean = rs_df[col].mean()\n",
    "        std = rs_df[col].std()\n",
    "        rs_df[col+'_cdf'] = norm.cdf(rs_df[col].values, loc=mean, scale=std)\n",
    "        rs_df[col+'_z'] = zscore(rs_df[col].values)       \n",
    "        rs_df[col+'_rank'] = rs_df[col].rank(method='min')\n",
    "        rs_df[col+'log'] = (df[col] + 1).apply(np.log)\n",
    "    return rs_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.corpus import stopwords\n",
    "def extract_topic(df):\n",
    "    rs_df = df\n",
    "    rs_df['hashtags'] = rs_df['hashtags'].apply(\n",
    "        lambda x: x.replace('[', '').replace(']', '').replace(\"'\", ''))\n",
    "    #join text and hashtags\n",
    "    rs_df['total_text'] = rs_df['text'] + ' ' + rs_df['hashtags']\n",
    "    vectorizer = TfidfVectorizer(min_df=1, max_features=None, stop_words=stopwords.words('french'))\n",
    "    vector = vectorizer.fit_transform(rs_df['text'])\n",
    "    svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "    svd.fit(vector)\n",
    "    topic=svd.transform(vector)\n",
    "    rs_df['topic_1'] = topic[:,0]\n",
    "    rs_df['topic_2'] = topic[:,1]\n",
    "    rs_df['topic_3'] = topic[:,2]\n",
    "    rs_df['topic_4'] = topic[:,3]\n",
    "    rs_df['topic_5'] = topic[:,4]\n",
    "    rs_df=rs_df.drop(['hashtags'],axis=1)\n",
    "    rs_df=rs_df.drop(['total_text'],axis=1)\n",
    "    return rs_df\n",
    "\n",
    "from textblob import TextBlob  # pip install textblob-fr\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer\n",
    "\n",
    "\n",
    "def sent_engineering(in_df):\n",
    "    rs_df = in_df\n",
    "    # add columns related to sentiment analysis\n",
    "    rs_df['polarity'] = rs_df['text'].apply(lambda x: TextBlob(\n",
    "        x, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[0])\n",
    "    rs_df['subjectivity'] = rs_df['text'].apply(lambda x: TextBlob(\n",
    "        x, pos_tagger=PatternTagger(), analyzer=PatternAnalyzer()).sentiment[1])\n",
    "    # drop the text column\n",
    "    rs_df = rs_df.drop(['text'], axis=1)\n",
    "    return rs_df\n",
    "\n",
    "def extract_url(in_df):\n",
    "    #count url\n",
    "    rs_df = in_df\n",
    "    rs_df['url_count'] = rs_df['urls'].apply(lambda x: len(x.split(',')))\n",
    "    rs_df=rs_df.drop(['urls'],axis=1)\n",
    "\n",
    "    return rs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "def extract_cluster(df,columns):\n",
    "    rs_df = df\n",
    "    rs_df['cluster'] = KMeans(n_clusters=100, random_state=0).fit_predict(rs_df[columns].values)\n",
    "    return rs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "new_X_train=extract_time_features(X_train)\n",
    "new_X_train=extract_ratio_features(new_X_train)\n",
    "new_X_train=extract_transfo(new_X_train,tweet_metrics_features)\n",
    "new_X_train=extract_topic(new_X_train)\n",
    "new_X_train=sent_engineering(new_X_train)\n",
    "new_X_train=extract_url(new_X_train)\n",
    "new_X_train=extract_cluster(new_X_train,['followers_count', 'friends_count', 'favorites_count','verified','statuses_count'])\n",
    "\n",
    "\n",
    "\n",
    "col=new_X_train.columns\n",
    "\n",
    "\n",
    "           \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X_test=extract_time_features(X_test)\n",
    "new_X_test=extract_ratio_features(new_X_test)\n",
    "new_X_test=extract_transfo(new_X_test,tweet_metrics_features)\n",
    "new_X_test=extract_topic(new_X_test)\n",
    "new_X_test=sent_engineering(new_X_test)\n",
    "new_X_test=extract_url(new_X_test)\n",
    "new_X_test=extract_cluster(new_X_test,['followers_count', 'friends_count', 'favorites_count','verified','statuses_count'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "   favorites_count  followers_count  statuses_count  friends_count  verified  \\\n",
      "0        -0.052491        -0.075595       -0.405109      -0.495929 -0.176266   \n",
      "\n",
      "    TweetID      hour       day  week_in_month  followers__favorites  ...  \\\n",
      "0  1.187134  0.379038  0.474938       0.735374              -0.01534  ...   \n",
      "\n",
      "   followers__friends__favoriteslog   topic_1   topic_2   topic_3   topic_4  \\\n",
      "0                          0.886959 -1.063357  0.026185  0.012034 -0.107607   \n",
      "\n",
      "    topic_5  polarity  subjectivity  url_count   cluster  \n",
      "0 -0.065893 -0.071362      -0.75764  -0.070227  0.853205  \n",
      "\n",
      "[1 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler#standard scaler seems to perform better\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(new_X_train)\n",
    "new_X_train_s = scaler.transform(new_X_train)\n",
    "new_X_test_s = scaler.transform(new_X_test)\n",
    "\n",
    "print(type(new_X_train_s))\n",
    "#array to pd\n",
    "new_X_train_s=pd.DataFrame(new_X_train_s, columns=col)\n",
    "new_X_test_s=pd.DataFrame(new_X_test_s, columns=col)\n",
    "#rindex columns\n",
    "new_X_train_s=new_X_train_s.reindex(columns=col)\n",
    "new_X_test_s=new_X_test_s.reindex(columns=col)\n",
    "print(new_X_train_s.head(1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_metrics_features = [\n",
    "    'followers_count', 'friends_count', 'favorites_count',\n",
    "    'followers__favorites', 'friends__favorites', 'followers__friends__favorites',\n",
    "]\n",
    "\n",
    "tweet_metrics_log_features = [feat+'_log' for feat in tweet_metrics_features]\n",
    "tweet_metrics_cdf_features = [feat+'_cdf' for feat in tweet_metrics_features]\n",
    "tweet_metrics_z_features = [feat+'_z' for feat in tweet_metrics_features]\n",
    "tweet_metrics_rank_features = [feat+'_rank' for feat in tweet_metrics_features]\n",
    "time_cat_features = ['hour', 'day', 'week_of_month']\n",
    "text_features = ['subjectivity', 'polarity', 'topic_1', 'topic_2', 'topic_3', 'topic_4', 'topic_5']\n",
    "other_features = ['url_count', 'cluster']\n",
    "\n",
    "\n",
    "remove=['TweetID']+tweet_metrics_rank_features+tweet_metrics_z_features+tweet_metrics_cdf_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping useless columns: ['TweetID', 'followers_count_rank', 'friends_count_rank', 'favorites_count_rank', 'followers__favorites_rank', 'friends__favorites_rank', 'followers__friends__favorites_rank', 'followers_count_z', 'friends_count_z', 'favorites_count_z', 'followers__favorites_z', 'friends__favorites_z', 'followers__friends__favorites_z', 'followers_count_cdf', 'friends_count_cdf', 'favorites_count_cdf', 'followers__favorites_cdf', 'friends__favorites_cdf', 'followers__friends__favorites_cdf']\n",
      "Training model...\n",
      "Epoch 1/12\n",
      "277/277 [==============================] - 10s 33ms/step - loss: 8.8314 - mae: 8.8314\n",
      "Epoch 2/12\n",
      "277/277 [==============================] - 9s 33ms/step - loss: 7.1946 - mae: 7.1946\n",
      "Epoch 3/12\n",
      "277/277 [==============================] - 9s 33ms/step - loss: 7.1615 - mae: 7.1615\n",
      "Epoch 4/12\n",
      "277/277 [==============================] - 9s 33ms/step - loss: 7.1109 - mae: 7.1109\n",
      "Epoch 5/12\n",
      " 91/277 [========>.....................] - ETA: 6s - loss: 6.5776 - mae: 6.5776"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\Projet  INF554\\INF554_kaggle\\DNN.ipynb Cellule 17\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X26sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTraining model...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X26sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m model \u001b[39m=\u001b[39m build_dnn_model(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X26sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     new_X_train_s\u001b[39m.\u001b[39mdrop(remove, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X26sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Projet%20%20INF554/INF554_kaggle/DNN.ipynb#X26sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m     new_X_train_s\u001b[39m.\u001b[39;49mdrop(remove,axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1210\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   1212\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[0;32m   1213\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[0;32m   1214\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m   1215\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1216\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1217\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1218\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    907\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 910\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    912\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    913\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:942\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    939\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    940\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    941\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 942\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    943\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    944\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    945\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3127\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   3128\u001b[0m   (graph_function,\n\u001b[0;32m   3129\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   3131\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1955\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1956\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1957\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1958\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1960\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1961\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m     args,\n\u001b[0;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1964\u001b[0m     executing_eagerly)\n\u001b[0;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    597\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    599\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    600\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    601\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    602\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    603\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    604\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    606\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    607\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    611\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\maria\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     59\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     60\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#import sequetial\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "import torch\n",
    "\n",
    "def build_dnn_model(input):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1024, input_dim=input.shape[1], activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "\n",
    "#convert to tensor selected data\n",
    "print('dropping useless columns:', remove)\n",
    "\n",
    "print('Training model...')\n",
    "model = build_dnn_model(\n",
    "    new_X_train_s.drop(remove, axis=1))\n",
    "history = model.fit(\n",
    "    new_X_train_s.drop(remove,axis=1), y_train, epochs=12, batch_size=512, verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd NN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.357897202613437\n"
     ]
    }
   ],
   "source": [
    "#import mlp\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "remove=['TweetID']\n",
    "model=MLPRegressor(hidden_layer_sizes=(8,8,8),activation='relu',\n",
    "                    solver='adam',alpha=0.001,batch_size='auto')\n",
    "model.fit(new_X_train.drop(remove, axis=1),y_train)\n",
    "y_pred = model.predict(new_X_test.drop(remove, axis=1))\n",
    "print(mean_absolute_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN with Pytorch\n",
    "-> Not finished yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Buid an MLP with pytoch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(44, 2048)\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "        self.fc4 = nn.Linear(128, 1)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        out = self.dropout1(out)\n",
    "\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.dropout2(out)\n",
    "\n",
    "        out = F.relu(self.fc3(out))\n",
    "        out = self.dropout3(out)\n",
    "\n",
    "        out = self.fc4(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "#convert to tensor\n",
    "X_train_tensor=torch.tensor(new_X_train.drop(remove, axis=1).values)\n",
    "X_test_tensor=torch.tensor(new_X_test.drop(remove, axis=1).values)\n",
    "y_train_tensor=torch.tensor(y_train.values)\n",
    "y_test_tensor=torch.tensor(y_test.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model class and assign to object\n",
    "model_dropout= DNN()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.RMSprop(model_dropout.parameters(), lr=1e-3)\n",
    "\n",
    "# Loss function\n",
    "criterion = torch.nn.L1Loss()\n",
    "\n",
    "(train_acc, val_acc, train_loss, val_loss) = train_model(model_dropout, optimizer, criterion)\n",
    "\n",
    "def plot_losses(history_train_loss, history_val_loss):\n",
    "    # Set plotting style\n",
    "    #plt.style.use(('dark_background', 'bmh'))\n",
    "    plt.style.use('bmh')\n",
    "    plt.rc('axes', facecolor='none')\n",
    "    plt.rc('figure', figsize=(16, 4))\n",
    "\n",
    "    # Plotting loss graph\n",
    "    plt.plot(history_train_loss, label='Train')\n",
    "    plt.plot(history_val_loss, label='Validation')\n",
    "    plt.title('Loss Graph')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_losses(train_loss, val_loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca02d05c83cb06a4d3d1bb3c2ad95bd9ee4b26f688526444572dc942a69d580d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
